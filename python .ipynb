{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_huggingface sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks_from_pdf(data_path, chunk_size, chunk_overlap):\n",
    "\n",
    "   '''\n",
    "   This function takes a directory of PDF files and creates chunks of text from each file.\n",
    "   The text is split into chunks of size `chunk_size` with an overlap of `chunk_overlap`.\n",
    "   This chunk is then converted into a langchain Document object.\n",
    "\n",
    "   Args:\n",
    "      data_path (str): The path to the directory containing the PDF files.\n",
    "      chunk_size (int): The size of each chunk.\n",
    "      chunk_overlap (int): The overlap between each chunk.\n",
    "\n",
    "   Returns:\n",
    "      docs (list): A list of langchain Document objects, each containing a chunk of text.\n",
    "   '''\n",
    "\n",
    "   # Load the documents from the directory\n",
    "   loader = DirectoryLoader(data_path, loader_cls=PyPDFLoader)\n",
    "\n",
    "   # Split the documents into chunks\n",
    "   text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=chunk_size,\n",
    "      chunk_overlap=chunk_overlap,\n",
    "      length_function=len,\n",
    "      is_separator_regex=False,\n",
    "   )\n",
    "   docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "   return docs\n",
    "\n",
    "data_path = \"D:\\RAG Project1\\data\"\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "docs = create_chunks_from_pdf(data_path, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set the environment variable for the cache directory to a valid path\n",
    "cache_dir = \"C:/Users/Bantu Sagar Kumar/transformers_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n",
    "\n",
    "# Create the cache directory if it doesn't exist\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# List of embedding models (make sure the model name exists on Hugging Face)\n",
    "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#embedding_model = 'BAAI/bge-large-en'\n",
    "\n",
    "\n",
    "# Load the embeddings model\n",
    "embeddings = SentenceTransformer(embedding_model, cache_folder=cache_dir)\n",
    "print(\"Embeddings model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_models = ['BAAI/bge-large-en']\n",
    "\n",
    "# Load the embeddings model\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG Project1\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\RAG Project1\\cache\\models--BAAI--bge-large-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=embedding_models[0], cache_folder='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-large-en', cache_folder='./cache', model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "def index_documents_and_retrieve(docs, embeddings):\n",
    "\n",
    "    '''\n",
    "    This function uses the Qdrant library to index the documents using the chunked text and embeddings model.\n",
    "    For the simplicity of the example, we are using in-memory storage only.\n",
    "\n",
    "    Args:\n",
    "    docs: List of documents generated from the document loader of langchain\n",
    "    embeddings: List of embeddings generated from the embeddings model\n",
    "\n",
    "    Returns:\n",
    "    retriever: Qdrant retriever object which can be used to retrieve the relevant documents\n",
    "    '''\n",
    "\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        docs,\n",
    "        embeddings,\n",
    "        location=\":memory:\",  # Local mode with in-memory storage only\n",
    "        collection_name=\"my_documents\",\n",
    "    )\n",
    "\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "retriever = index_documents_and_retrieve(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model_id = \"llama3:instruct\"\n",
    "\n",
    "# Load the Llama-3 model using the Ollama\n",
    "llm = ChatOllama(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def build_rag_chain(llm, retriever):\n",
    "\n",
    "    '''\n",
    "    This function builds the RAG chain using the LLM model and the retriever object. \n",
    "    The RAG chain is built using the following steps:\n",
    "    1. Retrieve the relevant documents using the retriever object\n",
    "    2. Pass the retrieved documents to the LLM model along with prompt generated using the context and question\n",
    "    3. Parse the output of the LLM model\n",
    "\n",
    "    Args:\n",
    "    llm: LLM model object\n",
    "    retriever: Qdrant retriever object\n",
    "\n",
    "    Returns:\n",
    "    rag_chain: RAG chain object which can be used to answer the questions based on the context\n",
    "    '''\n",
    "    \n",
    "    template = \"\"\"\n",
    "        Answer the question based only on the following context:\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\",\"question\"]\n",
    "        )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "rag_chain = build_rag_chain(llm, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke('What is this document about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
